<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Code Examples on Quantum Machine Learning Tutorial</title>
    <link>http://localhost:1313/code/</link>
    <description>Recent content in Code Examples on Quantum Machine Learning Tutorial</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/code/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Encoding</title>
      <link>http://localhost:1313/code/data-encode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/data-encode/</guid>
      <description>&lt;h2 id=&#34;Chapter2:preliminary-code&#34;&gt;Code Demonstration&lt;/h2&gt;&#xA;&lt;p&gt;This section provides code implementations for key techniques introduced&#xA;earlier, including quantum read-in strategies and block encoding, to&#xA;give readers the opportunity to practice and deepen their understanding.&lt;/p&gt;&#xA;&lt;h3 id=&#34;read-in-implementations&#34;&gt;Read-in implementations&lt;/h3&gt;&#xA;&lt;p&gt;This subsection demonstrates toy examples of implementing data encoding&#xA;methods in quantum computing, as discussed in earlier sections.&#xA;Specifically, we cover basis encoding, amplitude encoding, and angle&#xA;encoding. These examples aim to provide&#xA;readers with hands-on experience in applying quantum data encoding&#xA;techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Classification on MNIST</title>
      <link>http://localhost:1313/code/kernel-mnist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/kernel-mnist/</guid>
      <description>&lt;p&gt;In this tutorial, we will train a SVM classifier with a quantum kernel on the MNIST dataset. The basic pipeline is implemented with following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load the dataset.&lt;/li&gt;&#xA;&lt;li&gt;Define the quantum feature mapping.&lt;/li&gt;&#xA;&lt;li&gt;Construct the quantum kernel.&lt;/li&gt;&#xA;&lt;li&gt;Construct and train the SVM.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing all related libraries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pennylane &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; qml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fetch_openml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.decomposition &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; PCA&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; StandardScaler&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; accuracy_score&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then load the MNIST dataset. Here we focus on two classes of &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;, leading to a binary classification problem. PCA is firstly applied to reduce the feature dimension of images in MNIST to reduce the required number of qubits to encode this classifcal feature. The compressed features are then normalized to match the periodicity of the quantum feature mapping used later.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantum Classifier</title>
      <link>http://localhost:1313/code/classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/classifier/</guid>
      <description>&lt;p&gt;In this tutorial, we show how to utilize QNN to handle discriminative tasks based on PennyLane library. Specifically, we aim to implement a quantum binary classifier for the Wine dataset. The primary pipeline includes following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load and preprocess the Wine dataset.&lt;/li&gt;&#xA;&lt;li&gt;Implement a quantum read-in protocol to load classical data into quantum states.&lt;/li&gt;&#xA;&lt;li&gt;Construct a parameterized quantum circuit model to process the input.&lt;/li&gt;&#xA;&lt;li&gt;Train the whole circuit and test.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing all related libraries:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantum Patch GAN</title>
      <link>http://localhost:1313/code/patch-qgan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/patch-qgan/</guid>
      <description>&lt;p&gt;In this tutorial, we demonstrate how to implement a quantum patch GAN introduced in Chapter xxx for the generation of hand-written digit of five. The whole pipeline includes following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load and pre-process the dataset&lt;/li&gt;&#xA;&lt;li&gt;Build the classical discriminator&lt;/li&gt;&#xA;&lt;li&gt;Build the quantum generator&lt;/li&gt;&#xA;&lt;li&gt;Train the quantum patch GAN&lt;/li&gt;&#xA;&lt;li&gt;Visualize the generated images&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing required libraries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; nn&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.optim &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; optim&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch.utils.data &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Dataset, DataLoader&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pennylane &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; qml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-1-dataset-preparation&#34;&gt;Step 1: Dataset Preparation&lt;/h2&gt;&#xA;&lt;p&gt;We will use the &lt;a href=&#34;https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits&#34;&gt;Optical Recognition of Handwritten Digits dataset&lt;/a&gt;, where each data point represents an $8\times 8$ grayscale image. Letâ€™s start by defining a custom dataset class to load and process the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tranformer</title>
      <link>http://localhost:1313/code/transformer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/transformer/</guid>
      <description>&lt;p&gt;We explain how self-attention works with a simple concrete example.&#xA;Consider a short sequence of three words: &amp;ldquo;The cat sleeps&amp;rdquo;.&lt;/p&gt;&#xA;&lt;p&gt;First, we convert each word into an embedding vector. For this toy&#xA;example, we use very small 4-dimensional embeddings:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sleeps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In self-attention, each word needs to &amp;ldquo;attend&amp;rdquo; to all other words in the&#xA;sequence. This happens through three key steps using learned weight&#xA;matrices ($W_q$, $W_k$, $W_v$) to transform the embeddings into queries,&#xA;keys, and values. When we multiply our word embeddings by these&#xA;matrices, we get&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
