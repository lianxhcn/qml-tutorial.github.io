<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Chapter 5.4 Recent Advancements - Quantum Machine Learning Tutorial</title>
<meta name="description" content="A Hands-on Tutorial for Machine Learning Practitioners and Researchers">
<meta name="generator" content="Hugo 0.140.2">
<link href="https://qml-tutorial.github.io//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="https://qml-tutorial.github.io/chapter5/4/">
<link rel="stylesheet" href="https://qml-tutorial.github.io/css/theme.min.css">
<link rel="stylesheet" href="https://qml-tutorial.github.io/css/chroma.min.css">
<script defer src="https://qml-tutorial.github.io//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="https://qml-tutorial.github.io/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:url" content="https://qml-tutorial.github.io/chapter5/4/">
  <meta property="og:site_name" content="Quantum Machine Learning Tutorial">
  <meta property="og:title" content="Chapter 5.4 Recent Advancements">
  <meta property="og:description" content="Transformer architecture has profoundly revolutionized AI and has broad impacts. As classical computing approaches its physical limitations, it is important to ask how we can leverage quantum computers to advance Transformers with better performance and energy efficiency. Besides the fault-tolerant quantum Transformers, multiple works are advancing this frontier from various perspectives.
One aspect is to design novel quantum neural network architectures introduced in Chapter 4 with the intuition from the Transformer, especially the design of the self-attention block. In particular, @li2023quantumselfattentionneuralnetworks proposes the quantum self-attention neural networks and verifies their effectiveness with the synthetic data sets of quantum natural language processing. There are several follow-up works along this direction [@Cherrat_2024; @evans2024learningsasquatchnovelvariational; @widdows2024quantumnaturallanguageprocessing].">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chapter5">
    <meta property="og:image" content="https://qml-tutorial.github.io/images/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://qml-tutorial.github.io/images/og-image.png">
  <meta name="twitter:title" content="Chapter 5.4 Recent Advancements">
  <meta name="twitter:description" content="Transformer architecture has profoundly revolutionized AI and has broad impacts. As classical computing approaches its physical limitations, it is important to ask how we can leverage quantum computers to advance Transformers with better performance and energy efficiency. Besides the fault-tolerant quantum Transformers, multiple works are advancing this frontier from various perspectives.
One aspect is to design novel quantum neural network architectures introduced in Chapter 4 with the intuition from the Transformer, especially the design of the self-attention block. In particular, @li2023quantumselfattentionneuralnetworks proposes the quantum self-attention neural networks and verifies their effectiveness with the synthetic data sets of quantum natural language processing. There are several follow-up works along this direction [@Cherrat_2024; @evans2024learningsasquatchnovelvariational; @widdows2024quantumnaturallanguageprocessing].">

  <meta itemprop="name" content="Chapter 5.4 Recent Advancements">
  <meta itemprop="description" content="Transformer architecture has profoundly revolutionized AI and has broad impacts. As classical computing approaches its physical limitations, it is important to ask how we can leverage quantum computers to advance Transformers with better performance and energy efficiency. Besides the fault-tolerant quantum Transformers, multiple works are advancing this frontier from various perspectives.
One aspect is to design novel quantum neural network architectures introduced in Chapter 4 with the intuition from the Transformer, especially the design of the self-attention block. In particular, @li2023quantumselfattentionneuralnetworks proposes the quantum self-attention neural networks and verifies their effectiveness with the synthetic data sets of quantum natural language processing. There are several follow-up works along this direction [@Cherrat_2024; @evans2024learningsasquatchnovelvariational; @widdows2024quantumnaturallanguageprocessing].">
  <meta itemprop="wordCount" content="297">
  <meta itemprop="image" content="https://qml-tutorial.github.io/images/og-image.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '\\[', right: '\\]', display: true }, { left: '$', right: '$', display: false }, { left: '\\(', right: '\\)', display: false } ] });"></script>
</head>
<body><div class="container"><header>
<h1>Quantum Machine Learning Tutorial</h1>
<p class="description">A Hands-on Tutorial for Machine Learning Practitioners and Researchers</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li>
<li class=""><a href="/resources/">Resources</a></li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Chapter 5.4 Recent Advancements</h1>
<p>Transformer architecture has profoundly revolutionized AI and has broad
impacts. As classical computing approaches its physical limitations, it
is important to ask how we can leverage quantum computers to advance
Transformers with better performance and energy efficiency. Besides the
fault-tolerant quantum Transformers, multiple works are
advancing this frontier from various perspectives.</p>
<p>One aspect is to design novel quantum neural network architectures
introduced in Chapter 4 with the intuition from the Transformer,
especially the design of the self-attention block. In particular,
@li2023quantumselfattentionneuralnetworks proposes the quantum
self-attention neural networks and verifies their effectiveness with the
synthetic data sets of quantum natural language processing. There are
several follow-up works along this direction
[@Cherrat_2024; @evans2024learningsasquatchnovelvariational; @widdows2024quantumnaturallanguageprocessing].</p>
<p>Another research direction is exploring how to utilize quantum
processors to advance certain parts of the transformer architecture.
Specifically, @gao2023fastquantumalgorithmattention considers how to
compute the self-attention matrix under sparse assumption and shows a
quadratic quantum speedup.
@liu2024quantumcircuitbasedcompressionperspective harnesses quantum
neural networks to generate weight parameters for the classical model.
In addition, @liu2024towards devises a quantum algorithm for the
training process of large-scale neural networks, implying an exponential
speedup under certain conditions.</p>
<p>Despite the progress made, several important questions remain
unresolved. Among them, one key challenge is devising efficient methods
to encode classical data or parameters onto quantum computers.
Currently, most quantum algorithms can only implement one or at most
constant layers of Transformers
[@guo2024quantumlinear2024; @liao2024gptquantumcomputer; @khatri2024quixerquantumtransformermodel]
without quantum tomography. Are there effective methods that can be
generalized to multiple layers, or is achieving this even necessary?
Moreover, given the numerous variants of the classical Transformer
architecture, can these variants also benefit from the capabilities of
quantum computers? Lastly, if one considers training a model directly on
a quantum computer, is it possible to do so in a &ldquo;quantum-native&rdquo;
manner&mdash;avoiding excessive data read-in and read-out operations?</p>
<div class="edit-meta">

<br></div><nav class="pagination"><a class="nav nav-prev" href="https://qml-tutorial.github.io/chapter5/3/" title="Chapter 5.3 Runtime Analysis with Quadratic Speedups"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Chapter 5.3 Runtime Analysis with Quadratic Speedups</a>
<a class="nav nav-next" href="https://qml-tutorial.github.io/code/" title="Code Examples">Next - Code Examples <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="https://qml-tutorial.github.io/">Home</a></li>

<li class=""><a href="https://qml-tutorial.github.io/getting-started/">Getting Started</a>
  
</li>

<li class=""><a href="https://qml-tutorial.github.io/chapter1/">Chapter 1 Introduction of QML</a>
  
</li>

<li class=""><a href="https://qml-tutorial.github.io/chapter2/">Chapter 2 Basis of Quantum Computing</a>
  
<ul class="sub-menu">
<li class=""><a href="https://qml-tutorial.github.io/chapter2/1/">Chapter 2.1 From Classical Bits to Quantum Bits</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter2/2/">Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter2/3/">Chapter 2.3 Quantum Read-in and Read-out protocols</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter2/4/">Chapter 2.4 Quantum Linear Algebra</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter2/5/">Chapter 2.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="https://qml-tutorial.github.io/chapter3/">Chapter 3 Quantum Kernel Methods</a>
  
<ul class="sub-menu">
<li class=""><a href="https://qml-tutorial.github.io/chapter3/1/">Chapter 3.1 Classical Kernel</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter3/2/">Chapter 3.2 Quantum Kernel Machines</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter3/3/">Chapter 3.3 Theoretical Foundations</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter3/4/">Chapter 3.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="https://qml-tutorial.github.io/chapter4/">Chapter 4 Quantum Neural Networks</a>
  
<ul class="sub-menu">
<li class=""><a href="https://qml-tutorial.github.io/chapter4/1/">Chapter 4.1 Classical Neural Networks</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter4/2/">Chapter 4.2 Fault-tolerant Quantum Perceptron</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter4/3/">Chapter 4.3 Near-term quantum neural networks</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter4/4/">Chapter 4.4 Theoretical Foundations of QNNs</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter4/5/">Chapter 4.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class="parent"><a href="https://qml-tutorial.github.io/chapter5/">Chapter 5 Quantum Transformer</a>
  
<ul class="sub-menu">
<li class=""><a href="https://qml-tutorial.github.io/chapter5/1/">Chapter 5.1 Classical Transformer</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter5/2/">Chapter 5.2 Fault-tolerant Quantum Transformer</a></li>
<li class=""><a href="https://qml-tutorial.github.io/chapter5/3/">Chapter 5.3 Runtime Analysis with Quadratic Speedups</a></li>
<li class="active"><a href="https://qml-tutorial.github.io/chapter5/4/">Chapter 5.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="https://qml-tutorial.github.io/code/">Code Examples</a>
  
<ul class="sub-menu">
<li class=""><a href="https://qml-tutorial.github.io/code/data-encode/">Data Encoding</a></li>
<li class=""><a href="https://qml-tutorial.github.io/code/kernel-mnist/">Classification on MNIST</a></li>
<li class=""><a href="https://qml-tutorial.github.io/code/classifier/">Quantum Classifier</a></li>
<li class=""><a href="https://qml-tutorial.github.io/code/patch-qgan/">Quantum Patch GAN</a></li>
<li class=""><a href="https://qml-tutorial.github.io/code/transformer/">Tranformer</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
