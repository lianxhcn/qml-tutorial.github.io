<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Chapter 5.1 Classical Transformer - Quantum Machine Learning Tutorial</title>
<meta name="description" content="A Hands-on Tutorial for Machine Learning Practitioners and Researchers">
<meta name="generator" content="Hugo 0.140.2">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/chapter5/1/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:url" content="http://localhost:1313/chapter5/1/">
  <meta property="og:site_name" content="Quantum Machine Learning Tutorial">
  <meta property="og:title" content="Chapter 5.1 Classical Transformer">
  <meta property="og:description" content="This content of this section corresponds to the Chapter 5.1 of our paper. Please refer to the original paper for more details.
The transformer architecture is designed to predict the next token in a sequence by leveraging sophisticated neural network components. Its modular design—including residual connections, layer normalization, and feed-forward networks (FFNs) as introduced in Chapter 4.1—makes it highly scalable and customizable. This versatility has enabled its successful application in large-scale foundation models across diverse domains, including natural language processing, computer vision, reinforcement learning, robotics, and beyond.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chapter5">
    <meta property="og:image" content="http://localhost:1313/images/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/og-image.png">
  <meta name="twitter:title" content="Chapter 5.1 Classical Transformer">
  <meta name="twitter:description" content="This content of this section corresponds to the Chapter 5.1 of our paper. Please refer to the original paper for more details.
The transformer architecture is designed to predict the next token in a sequence by leveraging sophisticated neural network components. Its modular design—including residual connections, layer normalization, and feed-forward networks (FFNs) as introduced in Chapter 4.1—makes it highly scalable and customizable. This versatility has enabled its successful application in large-scale foundation models across diverse domains, including natural language processing, computer vision, reinforcement learning, robotics, and beyond.">

  <meta itemprop="name" content="Chapter 5.1 Classical Transformer">
  <meta itemprop="description" content="This content of this section corresponds to the Chapter 5.1 of our paper. Please refer to the original paper for more details.
The transformer architecture is designed to predict the next token in a sequence by leveraging sophisticated neural network components. Its modular design—including residual connections, layer normalization, and feed-forward networks (FFNs) as introduced in Chapter 4.1—makes it highly scalable and customizable. This versatility has enabled its successful application in large-scale foundation models across diverse domains, including natural language processing, computer vision, reinforcement learning, robotics, and beyond.">
  <meta itemprop="wordCount" content="125">
  <meta itemprop="image" content="http://localhost:1313/images/og-image.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '\\[', right: '\\]', display: true }, { left: '$', right: '$', display: false }, { left: '\\(', right: '\\)', display: false } ] });"></script>
</head>
<body>

<div class="container"><header>
<h1>Quantum Machine Learning Tutorial</h1>
<p class="description">A Hands-on Tutorial for Machine Learning Practitioners and Researchers</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Chapter 5.1 Classical Transformer</h1>
<p><em>This content of this section corresponds to the Chapter 5.1 of our paper. Please refer to the original paper for more details.</em></p>
<p>The transformer architecture is designed to predict the next <em>token</em> in a sequence by leveraging
sophisticated neural network components. Its modular design&mdash;including
residual connections, layer normalization, and feed-forward networks
(FFNs) as introduced in
Chapter 4.1&mdash;makes it highly scalable and
customizable. This versatility has enabled its successful application in
large-scale foundation models across diverse domains, including natural
language processing, computer vision, reinforcement learning, robotics,
and beyond.</p>
<p>The full architecture of Transformer is illustrated in
Figure 5.1. Note that while the original paper by
@vaswani2017attention introduced both encoder and decoder components,
contemporary large language models primarily adopt <em>decoder-only
architectures</em>, which have demonstrated superior practical performance.</p>
<figure style="text-align:center;">
  <img src="../../images/chapter5/transformer.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<div class="edit-meta">

<br><a href="https://github.com/thingsym/hugo-theme-techdoc/edit/master/chapter5/1.md" class="edit-page"><i class="fas fa-pen-square"></i>&nbsp;Edit on GitHub</a></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/chapter5/" title="Chapter 5 Quantum Transformer"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Chapter 5 Quantum Transformer</a>
<a class="nav nav-next" href="http://localhost:1313/chapter5/2/" title="Chapter 5.2 Fault-tolerant Quantum Transformer">Next - Chapter 5.2 Fault-tolerant Quantum Transformer <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="http://localhost:1313/">Home</a></li>

<li class=""><a href="http://localhost:1313/getting-started/">Getting Started</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter1/">Chapter 1 Introduction of QML</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter2/">Chapter 2 Basis of Quantum Computing</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter2/1/">Chapter 2.1 From Classical Bits to Quantum Bits</a></li>
<li class=""><a href="http://localhost:1313/chapter2/2/">Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</a></li>
<li class=""><a href="http://localhost:1313/chapter2/3/">Chapter 2.3 Quantum Read-in and Read-out protocols</a></li>
<li class=""><a href="http://localhost:1313/chapter2/4/">Chapter 2.4 Quantum Linear Algebra</a></li>
<li class=""><a href="http://localhost:1313/chapter2/5/">Chapter 2.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter3/">Chapter 3 Quantum Kernel Methods</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter3/1/">Chapter 3.1 Classical Kernel</a></li>
<li class=""><a href="http://localhost:1313/chapter3/2/">Chapter 3.2 Quantum Kernel Machines</a></li>
<li class=""><a href="http://localhost:1313/chapter3/3/">Chapter 3.3 Theoretical Foundations</a></li>
<li class=""><a href="http://localhost:1313/chapter3/4/">Chapter 3.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter4/">Chapter 4 Quantum Neural Networks</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter4/1/">Chapter 4.1 Classical Neural Networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/2/">Chapter 4.2 Fault-tolerant Quantum Perceptron</a></li>
<li class=""><a href="http://localhost:1313/chapter4/3/">Chapter 4.3 Near-term quantum neural networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/4/">Chapter 4.4 Theoretical Foundations of QNNs</a></li>
<li class=""><a href="http://localhost:1313/chapter4/5/">Chapter 4.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class="parent"><a href="http://localhost:1313/chapter5/">Chapter 5 Quantum Transformer</a>
  
<ul class="sub-menu">
<li class="active"><a href="http://localhost:1313/chapter5/1/">Chapter 5.1 Classical Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/2/">Chapter 5.2 Fault-tolerant Quantum Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/3/">Chapter 5.3 Runtime Analysis with Quadratic Speedups</a></li>
<li class=""><a href="http://localhost:1313/chapter5/4/">Chapter 5.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/code/">Code Examples</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/code/data-encode/">Data Encoding</a></li>
<li class=""><a href="http://localhost:1313/code/kernel-mnist/">Classification on MNIST</a></li>
<li class=""><a href="http://localhost:1313/code/classifier/">Quantum Classifier</a></li>
<li class=""><a href="http://localhost:1313/code/patch-qgan/">Quantum Patch GAN</a></li>
<li class=""><a href="http://localhost:1313/code/transformer/">Tranformer</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
