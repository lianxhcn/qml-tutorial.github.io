<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Chapter 5.3 Runtime Analysis with Quadratic Speedups - Quantum Machine Learning Tutorial</title>
<meta name="description" content="A Hands-on Tutorial for Machine Learning Practitioners and Researchers">
<meta name="generator" content="Hugo 0.140.2">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/chapter5/3/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:url" content="http://localhost:1313/chapter5/3/">
  <meta property="og:site_name" content="Quantum Machine Learning Tutorial">
  <meta property="og:title" content="Chapter 5.3 Runtime Analysis with Quadratic Speedups">
  <meta property="og:description" content="This content of this section corresponds to the Chapter 5.3 of our paper. Please refer to the original paper for more details.
As Theorem 5.1 shows, the quantum transformer uses ${\mathcal{\widetilde{O}}}(d N^2 \alpha_s\alpha_w)$ times the input block encodings, where $\alpha_s$ and $\alpha_w$ are encoding factors that significantly influence the overall complexity. We obtain this final complexity on the basis of the following considerations: for one-single layer transformer architecture, it includes one self-attention, one feed-forward network, and two residual connection with layer normalization, corresponding to the encoder-only or decoder-only structure. Starting from the input assumption, for the index $j\in [\ell]$, we first construct the block encoding of self-attention matrix. This output can be directly the input of the quantum residual connection and layer normalization, which output is a state encoding. The state encoding can directly be used as the input of the feed-forward network. Finally, we put the output of the feed-fordward network, which is a state encoding, into the residual connection block. This is possible by noticing that state encoding is a specific kind of the block encoding. Multiplying the query complexity of these computational blocks, one can achieve the result. The detailed analysis of runtime is referred to @guo2024quantumlinear2024">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chapter5">
    <meta property="og:image" content="http://localhost:1313/images/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/og-image.png">
  <meta name="twitter:title" content="Chapter 5.3 Runtime Analysis with Quadratic Speedups">
  <meta name="twitter:description" content="This content of this section corresponds to the Chapter 5.3 of our paper. Please refer to the original paper for more details.
As Theorem 5.1 shows, the quantum transformer uses ${\mathcal{\widetilde{O}}}(d N^2 \alpha_s\alpha_w)$ times the input block encodings, where $\alpha_s$ and $\alpha_w$ are encoding factors that significantly influence the overall complexity. We obtain this final complexity on the basis of the following considerations: for one-single layer transformer architecture, it includes one self-attention, one feed-forward network, and two residual connection with layer normalization, corresponding to the encoder-only or decoder-only structure. Starting from the input assumption, for the index $j\in [\ell]$, we first construct the block encoding of self-attention matrix. This output can be directly the input of the quantum residual connection and layer normalization, which output is a state encoding. The state encoding can directly be used as the input of the feed-forward network. Finally, we put the output of the feed-fordward network, which is a state encoding, into the residual connection block. This is possible by noticing that state encoding is a specific kind of the block encoding. Multiplying the query complexity of these computational blocks, one can achieve the result. The detailed analysis of runtime is referred to @guo2024quantumlinear2024">

  <meta itemprop="name" content="Chapter 5.3 Runtime Analysis with Quadratic Speedups">
  <meta itemprop="description" content="This content of this section corresponds to the Chapter 5.3 of our paper. Please refer to the original paper for more details.
As Theorem 5.1 shows, the quantum transformer uses ${\mathcal{\widetilde{O}}}(d N^2 \alpha_s\alpha_w)$ times the input block encodings, where $\alpha_s$ and $\alpha_w$ are encoding factors that significantly influence the overall complexity. We obtain this final complexity on the basis of the following considerations: for one-single layer transformer architecture, it includes one self-attention, one feed-forward network, and two residual connection with layer normalization, corresponding to the encoder-only or decoder-only structure. Starting from the input assumption, for the index $j\in [\ell]$, we first construct the block encoding of self-attention matrix. This output can be directly the input of the quantum residual connection and layer normalization, which output is a state encoding. The state encoding can directly be used as the input of the feed-forward network. Finally, we put the output of the feed-fordward network, which is a state encoding, into the residual connection block. This is possible by noticing that state encoding is a specific kind of the block encoding. Multiplying the query complexity of these computational blocks, one can achieve the result. The detailed analysis of runtime is referred to @guo2024quantumlinear2024">
  <meta itemprop="wordCount" content="694">
  <meta itemprop="image" content="http://localhost:1313/images/og-image.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '\\[', right: '\\]', display: true }, { left: '$', right: '$', display: false }, { left: '\\(', right: '\\)', display: false } ] });"></script>
</head>
<body>

<div class="container"><header>
<h1>Quantum Machine Learning Tutorial</h1>
<p class="description">A Hands-on Tutorial for Machine Learning Practitioners and Researchers</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li>
<li class=""><a href="/about/">About</a></li>
<li class=""><a href="/resources/">Resources</a></li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Chapter 5.3 Runtime Analysis with Quadratic Speedups</h1>
<p><em>This content of this section corresponds to the Chapter 5.3 of our paper. Please refer to the original paper for more details.</em></p>
<p>As Theorem 5.1 shows, the quantum transformer
uses ${\mathcal{\widetilde{O}}}(d N^2 \alpha_s\alpha_w)$ times the input
block encodings, where $\alpha_s$ and $\alpha_w$ are encoding factors
that significantly influence the overall complexity. We obtain this
final complexity on the basis of the following considerations: for
one-single layer transformer architecture, it includes one
self-attention, one feed-forward network, and two residual connection
with layer normalization, corresponding to the encoder-only or
decoder-only structure. Starting from the input assumption, for the index $j\in [\ell]$, we first
construct the block encoding of self-attention matrix. This output can be directly
the input of the quantum residual connection and layer normalization, which output is a state
encoding. The state encoding can directly be used
as the input of the feed-forward network. Finally, we put the output of the
feed-fordward network, which is a state encoding, into the residual
connection block. This is possible by noticing that state encoding is a
specific kind of the block encoding. Multiplying the query complexity of
these computational blocks, one can achieve the result. The detailed analysis of
runtime is referred to @guo2024quantumlinear2024</p>
<p>By analyzing naive matrix multiplication, the runtime of classical
single-layer transformer inference is $\mathcal{O}(\ell d + d^2)$, where $d$ is
the embedding dimension and $\ell=2^N$ is the input sequence length.</p>
<p>In this section, we provide a combined analytical and numerical analysis
to explore the potential of a quantum speedup in time complexity.</p>
<p>Recall that the encoding factor $\alpha$ is lower bounded by the
spectral norm of a block-encoded matrix $A$, i.e.,
$\alpha \geq \left|A\right|$. Given access to quantum Random Access Memory
(QRAM) and a quantum data structure
[@lloyd2014quantum; @kerenidisQuantumRecommendationSystems2016], there
are well-known implementations enable the construction of a block
encoding for an arbitrary matrix $A$ where the encoding factor is upper
bounded by the Frobenius norm $\left|A\right|_F$. We numerically study these
two norms of the input matrices of several open-source large language
models[^1].</p>
<p>We first investigate the input sequence matrix $S$, which introduces the
dependency on $\ell$. We consider input data in real-world applications
sampled from the widely-used Massive Multitask Language Understanding
(MMLU) dataset [@hendry2021measuring] covering 57 subjects across
science, technology, engineering, mathematics, the humanities, the
social sciences, and more. The scaling of the spectral norm and
Frobenius norm of $S$ on the MMLU dataset is demonstrated in
Figure 5.3. We can find that the matrix
norms of the input matrix of all LLMs scale at most as
$\mathcal{O}(\sqrt{\ell})$.</p>
<figure style="text-align:center;">
  <img src="../../images/chapter5/fig5-3.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<p>As additional interest, this analysis of the matrix norm provides new
insights for the classical tokenization and embedding design. We also
observe that comparatively more advanced LLM models like <em>Llama2-7b</em> and
<em>Mistral-7b</em> have large variances of matrix norms. This phenomenon is
arguably the consequence of the training in those models; the embeddings
that frequently appear in the real-world dataset are actively updated at
the pre-training stage, and therefore, are more broadly distributed.</p>
<p>We then compute the spectral and Frobenius norms of weight matrices
($W_q,W_k,W_v$) for the large language models. The result can be seen in Figure 5.4. Many of the LLMs below a dimension $d$ of
$10^3$ that we have checked have substantially different norms. We
observe that for larger models such as <em>Llama2-7b</em> and <em>Mistral-7b</em>,
which are close to the current state-of-the-art open-source models, the
norms do not change dramatically. Therefore, it is reasonable to assume
that the spectral norm and the Frobenius norm of the weight matrices are
at most $\mathcal{O}(\sqrt{d})$ for advanced LLMs.</p>
<figure style="text-align:center;">
  <img src="../../images/chapter5/fig5-4.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<p>Given these numerical experiments it is reasonable to assume that
$\alpha_s = \mathcal{O}(\sqrt{\ell})$ and $\alpha_w = \mathcal{O}(\sqrt{d})$, and we
obtain a query complexity of the quantum transformer in
$\widetilde{\mathcal{O}}(d^{\frac{3}{2}} \sqrt{\ell})$. We continue with
a discussion of the possible time complexity. With the QRAM assumption,
the input block encodings can be implemented in polylog time of $\ell$.
Even without a QRAM assumption there can be cases when the input
sequence is generated efficiently, for example when the sequence is
generated from a differential equation, see additional discussions in
the supplementary material. In these cases, we demonstrate that a
quadratic speedup of the runtime of a single-layer transformer can be
expected.</p>
<div class="edit-meta">

<br><a href="https://github.com/thingsym/hugo-theme-techdoc/edit/master/chapter5/3.md" class="edit-page"><i class="fas fa-pen-square"></i>&nbsp;Edit on GitHub</a></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/chapter5/2/" title="Chapter 5.2 Fault-tolerant Quantum Transformer"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Chapter 5.2 Fault-tolerant Quantum Transformer</a>
<a class="nav nav-next" href="http://localhost:1313/chapter5/4/" title="Chapter 5.4 Recent Advancements">Next - Chapter 5.4 Recent Advancements <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="http://localhost:1313/">Home</a></li>

<li class=""><a href="http://localhost:1313/getting-started/">Getting Started</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter1/">Chapter 1 Introduction of QML</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter2/">Chapter 2 Basis of Quantum Computing</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter2/1/">Chapter 2.1 From Classical Bits to Quantum Bits</a></li>
<li class=""><a href="http://localhost:1313/chapter2/2/">Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</a></li>
<li class=""><a href="http://localhost:1313/chapter2/3/">Chapter 2.3 Quantum Read-in and Read-out protocols</a></li>
<li class=""><a href="http://localhost:1313/chapter2/4/">Chapter 2.4 Quantum Linear Algebra</a></li>
<li class=""><a href="http://localhost:1313/chapter2/5/">Chapter 2.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter3/">Chapter 3 Quantum Kernel Methods</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter3/1/">Chapter 3.1 Classical Kernel</a></li>
<li class=""><a href="http://localhost:1313/chapter3/2/">Chapter 3.2 Quantum Kernel Machines</a></li>
<li class=""><a href="http://localhost:1313/chapter3/3/">Chapter 3.3 Theoretical Foundations</a></li>
<li class=""><a href="http://localhost:1313/chapter3/4/">Chapter 3.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter4/">Chapter 4 Quantum Neural Networks</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter4/1/">Chapter 4.1 Classical Neural Networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/2/">Chapter 4.2 Fault-tolerant Quantum Perceptron</a></li>
<li class=""><a href="http://localhost:1313/chapter4/3/">Chapter 4.3 Near-term quantum neural networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/4/">Chapter 4.4 Theoretical Foundations of QNNs</a></li>
<li class=""><a href="http://localhost:1313/chapter4/5/">Chapter 4.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class="parent"><a href="http://localhost:1313/chapter5/">Chapter 5 Quantum Transformer</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter5/1/">Chapter 5.1 Classical Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/2/">Chapter 5.2 Fault-tolerant Quantum Transformer</a></li>
<li class="active"><a href="http://localhost:1313/chapter5/3/">Chapter 5.3 Runtime Analysis with Quadratic Speedups</a></li>
<li class=""><a href="http://localhost:1313/chapter5/4/">Chapter 5.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/code/">Code Examples</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/code/data-encode/">Data Encoding</a></li>
<li class=""><a href="http://localhost:1313/code/kernel-mnist/">Classification on MNIST</a></li>
<li class=""><a href="http://localhost:1313/code/classifier/">Quantum Classifier</a></li>
<li class=""><a href="http://localhost:1313/code/patch-qgan/">Quantum Patch GAN</a></li>
<li class=""><a href="http://localhost:1313/code/transformer/">Tranformer</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
