<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Chapter 4.4 Theoretical Foundations of QNNs - Quantum Machine Learning Tutorial</title>
<meta name="description" content="A Hands-on Tutorial for Machine Learning Practitioners and Researchers">
<meta name="generator" content="Hugo 0.140.2">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/chapter4/4/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:url" content="http://localhost:1313/chapter4/4/">
  <meta property="og:site_name" content="Quantum Machine Learning Tutorial">
  <meta property="og:title" content="Chapter 4.4 Theoretical Foundations of QNNs">
  <meta property="og:description" content="This content of this section corresponds to the Chapter 4.4 of our paper. Please refer to the original paper for more details.
As a machine learning (ML) model, the primary goal of quantum neural networks (QNNs) is to make accurate predictions on unseen data. Achieving this goal depends on three key factors: expressivity, generalization ability, and trainability. A thorough understanding of these factors is crucial for evaluating the potential advantages and limitations of QNNs compared to classical ML models. Instead of providing an exhaustive review of all theoretical results, this section focuses on near-term QNNs, emphasizing key conceptual insights into their capabilities and limitations.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chapter4">
    <meta property="og:image" content="http://localhost:1313/images/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/og-image.png">
  <meta name="twitter:title" content="Chapter 4.4 Theoretical Foundations of QNNs">
  <meta name="twitter:description" content="This content of this section corresponds to the Chapter 4.4 of our paper. Please refer to the original paper for more details.
As a machine learning (ML) model, the primary goal of quantum neural networks (QNNs) is to make accurate predictions on unseen data. Achieving this goal depends on three key factors: expressivity, generalization ability, and trainability. A thorough understanding of these factors is crucial for evaluating the potential advantages and limitations of QNNs compared to classical ML models. Instead of providing an exhaustive review of all theoretical results, this section focuses on near-term QNNs, emphasizing key conceptual insights into their capabilities and limitations.">

  <meta itemprop="name" content="Chapter 4.4 Theoretical Foundations of QNNs">
  <meta itemprop="description" content="This content of this section corresponds to the Chapter 4.4 of our paper. Please refer to the original paper for more details.
As a machine learning (ML) model, the primary goal of quantum neural networks (QNNs) is to make accurate predictions on unseen data. Achieving this goal depends on three key factors: expressivity, generalization ability, and trainability. A thorough understanding of these factors is crucial for evaluating the potential advantages and limitations of QNNs compared to classical ML models. Instead of providing an exhaustive review of all theoretical results, this section focuses on near-term QNNs, emphasizing key conceptual insights into their capabilities and limitations.">
  <meta itemprop="wordCount" content="1050">
  <meta itemprop="image" content="http://localhost:1313/images/og-image.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '\\[', right: '\\]', display: true }, { left: '$', right: '$', display: false }, { left: '\\(', right: '\\)', display: false } ] });"></script>
</head>
<body>

<div class="container"><header>
<h1>Quantum Machine Learning Tutorial</h1>
<p class="description">A Hands-on Tutorial for Machine Learning Practitioners and Researchers</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li>
<li class=""><a href="/resources/">Resources</a></li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Chapter 4.4 Theoretical Foundations of QNNs</h1>
<p><em>This content of this section corresponds to the Chapter 4.4 of our paper. Please refer to the original paper for more details.</em></p>
<p>As a machine learning (ML) model, the primary goal of quantum neural
networks (QNNs) is to make accurate predictions on unseen data.
Achieving this goal depends on three key factors: expressivity,
generalization ability, and trainability. A thorough understanding of
these factors is crucial for evaluating the potential advantages and
limitations of QNNs compared to classical ML models. Instead of
providing an exhaustive review of all theoretical results, this section
focuses on near-term QNNs, emphasizing key conceptual insights into
their capabilities and limitations.</p>
<h3 id="chapt5:sec:expr_gene">Expressivity and generalization of quantum neural networks</h3>
<p>The expressivity and generalization are deeply interconnected within the
framework of statistical learning theory for understanding the
prediction ability of any learning model. To better understand these two
terms in the context of quantum neural networks, we first review the
framework of empirical risk minimization (ERM), which is a popular
framework for analyzing these abilities in statistical learning theory.</p>
<p>Consider the training dataset
$\mathcal{D} = \{(\bm{x}^{(i)},\bm{y}^{(i)})\} \in \mathcal{X} \times \mathcal{Y}$
sampled independentaly from an unknown distribution $\mathcal{P}$, a
learning algorithm $\mathcal{A}$ aims to use the dataset $\mathcal{D}$
to infer a hypotheis $h_{\bm{\theta}^*}: \mathcal{X}\to \mathcal{Y}$
from the hypothesis space $\mathcal{H}$ that could accurately predict
all labels of $\bm{x}\in \mathcal{X}$ following the distribution
$\mathcal{P}$. Under the framework of ERM, this is equivalent to
identifying an optimal hypothesis in $\mathcal{H}$ minimizing the
expected risk</p>
<figure style="text-align:center;">
  <img src="../../images/chapter4/empirical_risk.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<p>where $\ell(\cdot,\cdot)$ refers to the per-sample loss predefined by
the learner. Unfortunately, the inaccessible distribution $\mathcal{P}$
forbids us to assess the expected risk directly. In practice,
$\mathcal{A}$ alternatively learns an empirical hypothesis
$h_{\hat{\bm{\theta}}} \in \mathcal{H}$, as the global minimizer of the
(regularized) loss function
$$\mathcal{L}(\bm{\theta}, \mathcal{D}) = \frac{1}{|\mathcal{D}|} \sum_{i=1}^{|\mathcal{D}|} \ell (h_{\bm{\theta}}(\bm{x}^{(i)}), \bm{y}^{(i)}) + \mathcal{R}(\bm{\theta}),$$
where $\mathcal{R}(\bm{\theta})$ refers to an optional regularizer, as
will be detailed in the following. Moreover, the first term on the
right-hand side refers to the empirical risk
$$R_{\text{ERM}}(h_{\hat{\bm{\theta}}})= \frac{1}{|\mathcal{D}|} \sum_{i=1}^{|\mathcal{D}|} \ell (h_{\hat{\bm{\theta}}}(\bm{x}^{(i)}), \bm{y}^{(i)}),$$
which is also known as the training error. To address the intractability
of $R(h_{\hat{\bm{\theta}}})$, one can decompose it into two measurable
terms, $$
R(h_{\hat{\bm{\theta}}}) = R_{\text{ERM}}(h_{\hat{\bm{\theta}}}) + R_{\text{Gene}}(h_{\hat{\bm{\theta}}}),$$
where
$R_{\text{Gene}}(h_{\hat{\bm{\theta}}}) = R(h_{\hat{\bm{\theta}}}) - R_{\text{ERM}}(h_{\hat{\bm{\theta}}})$
refers to the generalization error. In this regard, achieving a small
prediction error requires the learning model to achieve both a small
training error and a small generalization error.</p>
<h4 id="expressivity-of-qnns">Expressivity of QNNs</h4>
<p>In this chapter, we analyze the generalization error of QNNs through a
specific measure of model complexity: the covering number. By leveraging
this measure, we aim to understand better and characterize the
generalization performance of QNNs.</p>
<p>To elucidate the specific definition of the covering number, we first
review the general structures of QNNs. Define
$\rho\in \mathbb{C}^{2^N \times 2^N}$ as the $N$-qubit input quantum
states, $O\in  \mathbb{C}^{2^N \times 2^N}$ as the quantum observable,
$U(\bm{\theta})=\prod_{l=1}^{N_g} u_{l}(\bm{\theta}) \in \mathcal{U}(2^N)$
as the applied ansatz, where $\bm{\theta}\in \Theta$ are the trainable
parameters living in the parameter space $\Theta$,
$u_{l}(\bm{\theta}) \in \mathcal{U}(2^k)$ refers to the $l$-th quantum
gate operated with at most $k$-qubits with $k\le N$, and
$\mathcal{U}(2^N)$ stands for the unitary group in dimension $2^N$. In
general, $U(\bm{\theta})$ is formed by $N_{gt}$ trainable gates and
$N_g-N_{gt}$ fixed gates, e.g., $\Theta \subset [0, 2\pi)^{N_{gt}}$.
Under the above definitions, the explicit form of the output of the
quantum circuit under the noiseless scenarios is
$$h(\bm{\theta},O,\rho):= \text{Tr}\left( U(\bm{\theta})^{\dagger} O U(\bm{\theta}) \rho \right).$$
Given the training data set
${(\rho^{(i)}, \bm{y}^{(i)})}$ and loss function
$\mathcal{L}(\cdot)$, the quantum neural network is optimized to find a
good approximation
$h^*(\bm{\theta},O,\rho)= \arg \min_{h(\bm{\theta},O,\rho) \in \mathcal{H}}$
that can well approximate the target concept, where $\mathcal{H}$ refers
to the hypothesis space of QNNs with</p>
<figure style="text-align:center;">
  <img src="../../images/chapter4/hypo.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<p>When $\mathcal{H}$ has a modest
size and covers the target concepts, the estimated hypothesis could well
approximate the target concept. By contrast, when the complexity of
$\mathcal{H}$ is too low, there exists a large gap between the estimated
hypothesis and the target concept. An effective measure to evaluate the
complexity of $\mathcal{H}$ is covering number shown in Figure 4.11, an advanced tool broadly
used in statistical learning theory, to bound the complexity of
$\mathcal{H}$ and measure the expressivity of QNNs.</p>
<figure style="text-align:center;">
  <img src="../../images/chapter4/covering_number.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<h4 id="generalization-error-of-qnns">Generalization error of QNNs</h4>
<p>As the relation between generalization error and covering number is
well-established in statistical learning theory, we can directly obtain
the generalization error bound with the above bounds of covering number
following the same conventions.</p>
<p><em>Theorem 4.6.</em>
Assume that the loss function $\ell$ is $L_1$-Lipschitz and upper bounded
by a constant $C$, the QNN-based learning algorithm outputs a hypothesis
$h_{\hat{\bm{\theta}}}$ from the training dataset $\mathcal{S}$ of size
$n$. Following the notations of risk
$R_{\text{Gene}}(h_{\hat{\bm{\theta}}})=R(h_{\hat{\bm{\theta}}})-R_{\text{ERM}}(h_{\hat{\bm{\theta}}})$, for $0&lt;\epsilon&lt;1/10$, with
probability at least $1-\delta$ with $\delta \in (0,1)$, we have</p>
<p>$$R_{\text{Gene}}(h_{\hat{\bm{\theta}}}) \le \mathcal{O}\left(\frac{8L+c+24L \sqrt{N_{gt}}\cdot 2^k}{\sqrt{n}} \right).$$</p>
<p>The assumption used in this analysis is quite mild, as the loss
functions in QNNs are generally Lipschitz continuous and can be bounded
above by a constant $C$. This property has been broadly employed to
understand the capability of QNNs. The results obtained have three key
implications. First, the generalization bound exhibits an exponential
dependence on the term $k$ and a sublinear dependence on the number of
trainable quantum gates $N_{gt}$. This observation reflects the quantum
version of Occam&rsquo;s razor [@haussler1987occam], where the parsimony of
the output hypothesis implies greater predictive power. Second,
increasing the number of training examples $n$ improves the
generalization bound. This suggests that incorporating more training
data is essential for optimizing complex quantum circuits. Lastly, the
sublinear dependence on $N_{gt}$ may limit our ability to accurately
assess the generalization performance of overparameterized QNNs
[@larocca2023theory]. Together, these implications provide valuable
insights for designing more powerful QNNs.</p>
<h3 id="chapt5:sec:trainability">Trainability of quantum neural networks</h3>
<p>The parameters in QNNs are trained using gradient-based optimizers such
as gradient descent. Therefore, the magnitude of the gradient plays a
crucial role in the trainability of QNNs. Specifically, large gradients
are desirable, as they allow the loss function to decrease rapidly and
consistently. However, this favorable property does not hold across a
wide range of problem settings. In contrast, training QNNs usually
encounters the barren plateau (BP) problem [@mcclean2018barren], , the
variance of the gradient, on average, decreases exponentially as the
number of qubits increases. In this section, we first introduce an
example demonstrating how quantum circuits that form unitary
$2$-designs [@dankert2009exact] lead to BP, and then discuss several
techniques to avoid or mitigate its impact.</p>
<div class="edit-meta">

<br><a href="https://github.com/thingsym/hugo-theme-techdoc/edit/master/chapter4/4.md" class="edit-page"><i class="fas fa-pen-square"></i>&nbsp;Edit on GitHub</a></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/chapter4/3/" title="Chapter 4.3 Near-term quantum neural networks"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Chapter 4.3 Near-term quantum neural networks</a>
<a class="nav nav-next" href="http://localhost:1313/chapter4/5/" title="Chapter 4.5 Recent Advancements">Next - Chapter 4.5 Recent Advancements <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="http://localhost:1313/">Home</a></li>

<li class=""><a href="http://localhost:1313/getting-started/">Getting Started</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter1/">Chapter 1 Introduction of QML</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter2/">Chapter 2 Basis of Quantum Computing</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter2/1/">Chapter 2.1 From Classical Bits to Quantum Bits</a></li>
<li class=""><a href="http://localhost:1313/chapter2/2/">Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</a></li>
<li class=""><a href="http://localhost:1313/chapter2/3/">Chapter 2.3 Quantum Read-in and Read-out protocols</a></li>
<li class=""><a href="http://localhost:1313/chapter2/4/">Chapter 2.4 Quantum Linear Algebra</a></li>
<li class=""><a href="http://localhost:1313/chapter2/5/">Chapter 2.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter3/">Chapter 3 Quantum Kernel Methods</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter3/1/">Chapter 3.1 Classical Kernel</a></li>
<li class=""><a href="http://localhost:1313/chapter3/2/">Chapter 3.2 Quantum Kernel Machines</a></li>
<li class=""><a href="http://localhost:1313/chapter3/3/">Chapter 3.3 Theoretical Foundations</a></li>
<li class=""><a href="http://localhost:1313/chapter3/4/">Chapter 3.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class="parent"><a href="http://localhost:1313/chapter4/">Chapter 4 Quantum Neural Networks</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter4/1/">Chapter 4.1 Classical Neural Networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/2/">Chapter 4.2 Fault-tolerant Quantum Perceptron</a></li>
<li class=""><a href="http://localhost:1313/chapter4/3/">Chapter 4.3 Near-term quantum neural networks</a></li>
<li class="active"><a href="http://localhost:1313/chapter4/4/">Chapter 4.4 Theoretical Foundations of QNNs</a></li>
<li class=""><a href="http://localhost:1313/chapter4/5/">Chapter 4.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter5/">Chapter 5 Quantum Transformer</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter5/1/">Chapter 5.1 Classical Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/2/">Chapter 5.2 Fault-tolerant Quantum Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/3/">Chapter 5.3 Runtime Analysis with Quadratic Speedups</a></li>
<li class=""><a href="http://localhost:1313/chapter5/4/">Chapter 5.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/code/">Code Examples</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/code/data-encode/">Data Encoding</a></li>
<li class=""><a href="http://localhost:1313/code/kernel-mnist/">Classification on MNIST</a></li>
<li class=""><a href="http://localhost:1313/code/classifier/">Quantum Classifier</a></li>
<li class=""><a href="http://localhost:1313/code/patch-qgan/">Quantum Patch GAN</a></li>
<li class=""><a href="http://localhost:1313/code/transformer/">Tranformer</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
