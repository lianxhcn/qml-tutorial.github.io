<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Quantum Machine Learning Tutorial</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Quantum Machine Learning Tutorial</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Encoding</title>
      <link>http://localhost:1313/code/data-encode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/data-encode/</guid>
      <description>&lt;h2 id=&#34;Chapter2:preliminary-code&#34;&gt;Code Demonstration&lt;/h2&gt;&#xA;&lt;p&gt;This section provides code implementations for key techniques introduced&#xA;earlier, including quantum read-in strategies and block encoding, to&#xA;give readers the opportunity to practice and deepen their understanding.&lt;/p&gt;&#xA;&lt;h3 id=&#34;read-in-implementations&#34;&gt;Read-in implementations&lt;/h3&gt;&#xA;&lt;p&gt;This subsection demonstrates toy examples of implementing data encoding&#xA;methods in quantum computing, as discussed in earlier sections.&#xA;Specifically, we cover basis encoding, amplitude encoding, and angle&#xA;encoding. These examples aim to provide&#xA;readers with hands-on experience in applying quantum data encoding&#xA;techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Classification on MNIST</title>
      <link>http://localhost:1313/code/kernel-mnist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/kernel-mnist/</guid>
      <description>&lt;p&gt;In this tutorial, we will train a SVM classifier with a quantum kernel on the MNIST dataset. The basic pipeline is implemented with following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load the dataset.&lt;/li&gt;&#xA;&lt;li&gt;Define the quantum feature mapping.&lt;/li&gt;&#xA;&lt;li&gt;Construct the quantum kernel.&lt;/li&gt;&#xA;&lt;li&gt;Construct and train the SVM.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing all related libraries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pennylane &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; qml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fetch_openml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.decomposition &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; PCA&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; StandardScaler&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; accuracy_score&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then load the MNIST dataset. Here we focus on two classes of &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;, leading to a binary classification problem. PCA is firstly applied to reduce the feature dimension of images in MNIST to reduce the required number of qubits to encode this classifcal feature. The compressed features are then normalized to match the periodicity of the quantum feature mapping used later.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantum Classifier</title>
      <link>http://localhost:1313/code/classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/classifier/</guid>
      <description>&lt;p&gt;In this tutorial, we show how to utilize QNN to handle discriminative tasks based on PennyLane library. Specifically, we aim to implement a quantum binary classifier for the Wine dataset. The primary pipeline includes following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load and preprocess the Wine dataset.&lt;/li&gt;&#xA;&lt;li&gt;Implement a quantum read-in protocol to load classical data into quantum states.&lt;/li&gt;&#xA;&lt;li&gt;Construct a parameterized quantum circuit model to process the input.&lt;/li&gt;&#xA;&lt;li&gt;Train the whole circuit and test.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing all related libraries:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2.1 From Classical Bits to Quantum Bits</title>
      <link>http://localhost:1313/chapter2/1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter2/1/</guid>
      <description>&lt;p&gt;In this section, we define quantum bits (qubits) and present the&#xA;mathematical tools used to describe quantum states. We begin by&#xA;discussing classical bits and then transition to their quantum&#xA;counterparts.&lt;/p&gt;&#xA;&lt;h3 id=&#34;classical-bits&#34;&gt;Classical bits&lt;/h3&gt;&#xA;&lt;p&gt;In classical computing, a bit is the basic unit of information, which&#xA;can exist in one of two distinct states: $0$ or $1$. Each bit holds a&#xA;definite value at any given time. When multiple classical bits are used&#xA;together, they can represent more complex information. For instance, a&#xA;set of three bits can represent $2^3=8$ distinct states, ranging from&#xA;$000$ to $111$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3.1 Classical Kernel</title>
      <link>http://localhost:1313/chapter3/1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter3/1/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 3.1 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;motivations-for-classical-kernel-machines&#34;&gt;Motivations for classical kernel machines&lt;/h3&gt;&#xA;&lt;p&gt;Before delving into kernel machines, it is essential to first understand&#xA;the motivation behind kernel methods. In many machine learning tasks,&#xA;particularly in classification, the goal is to find a decision boundary&#xA;that best separates different classes of data. When the data is linearly&#xA;separable, this boundary can be represented as a straight line (in 2D),&#xA;a plane (in 3D), or a hyperplane (in higher dimensions), as illustrated&#xA;in the following Figure. Mathematically, given an input space&#xA;$\mathcal{X}\subset \mathbb{R}^d$ with $d\ge 1$ and a target or output&#xA;space $\mathcal{Y}={+1,-1}$, we consider a training dataset&#xA;$\mathcal{D}={(\bm{x}^{(i)}, {y}^{(i)})}_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$&#xA;where each data point $\bm{x}^{(i)}  \in \mathcal{X}$ is associated with&#xA;a label ${y}^{(i)}  \in \mathcal{Y}$. For the dataset to be linearly&#xA;separable, there must exist a vector $\bm{w} \in \mathbb{R}^{d}$ and a&#xA;bias term $b\in \mathbb{R}$ such that&#xA;$$\forall i\in [n], \quad {y}^{(i)}(\bm{w}^{\top} \cdot \bm{x}^{(i)}+b)\ge 0.$$&#xA;This means that a hyperplane defined by $(\bm{\omega},b)$ can perfectly&#xA;separate the two classes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4.1 Classical Neural Networks</title>
      <link>http://localhost:1313/chapter4/1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter4/1/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 4.1 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Neural&#xA;networks [@mcculloch1943logical; @gardner1998artificial; @vaswani2017attention]&#xA;are computer models inspired by the structure of the human brain,&#xA;designed to process and analyze complex patterns in data. Originally&#xA;developed from the concept of neurons connected by weighted pathways,&#xA;neural networks have become one of the most powerful tools in artificial&#xA;intelligence [@lecun2015deep]. Each neuron processes its inputs by&#xA;applying weights and non-linear activations, producing an output that&#xA;feeds into the next layer of neurons. This structure enables neural&#xA;networks to learn complex functions during training [@hornik1993some].&#xA;For example, given a dataset of images and their labels, a neural&#xA;network can learn to classify categories, such as distinguishing between&#xA;cats and dogs, by adjusting its parameters during training. Guided by&#xA;optimization algorithms such as gradient&#xA;descent [@amari1993backpropagation], the learning process allows the&#xA;network to gradually reduce the error between the predicted and actual&#xA;outputs, allowing it to learn the best parameters for the given task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5.1 Classical Transformer</title>
      <link>http://localhost:1313/chapter5/1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter5/1/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 5.1 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;The transformer architecture is designed to predict the next &lt;em&gt;token&lt;/em&gt; in a sequence by leveraging&#xA;sophisticated neural network components. Its modular design&amp;mdash;including&#xA;residual connections, layer normalization, and feed-forward networks&#xA;(FFNs) as introduced in&#xA;Chapter 4.1&amp;mdash;makes it highly scalable and&#xA;customizable. This versatility has enabled its successful application in&#xA;large-scale foundation models across diverse domains, including natural&#xA;language processing, computer vision, reinforcement learning, robotics,&#xA;and beyond.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantum Patch GAN</title>
      <link>http://localhost:1313/code/patch-qgan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/patch-qgan/</guid>
      <description>&lt;p&gt;In this tutorial, we demonstrate how to implement a quantum patch GAN introduced in Chapter xxx for the generation of hand-written digit of five. The whole pipeline includes following steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load and pre-process the dataset&lt;/li&gt;&#xA;&lt;li&gt;Build the classical discriminator&lt;/li&gt;&#xA;&lt;li&gt;Build the quantum generator&lt;/li&gt;&#xA;&lt;li&gt;Train the quantum patch GAN&lt;/li&gt;&#xA;&lt;li&gt;Visualize the generated images&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We begin by importing required libraries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; nn&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.optim &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; optim&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch.utils.data &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Dataset, DataLoader&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pennylane &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; qml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-1-dataset-preparation&#34;&gt;Step 1: Dataset Preparation&lt;/h2&gt;&#xA;&lt;p&gt;We will use the &lt;a href=&#34;https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits&#34;&gt;Optical Recognition of Handwritten Digits dataset&lt;/a&gt;, where each data point represents an $8\times 8$ grayscale image. Let’s start by defining a custom dataset class to load and process the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</title>
      <link>http://localhost:1313/chapter2/2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter2/2/</guid>
      <description>&lt;p&gt;To process quantum states, we need to introduce quantum computation, a&#xA;fundamental model of which is the quantum circuit model. In this&#xA;section, we will begin with classical computation and transit to details about the&#xA;quantum circuit model, including quantum gates, quantum channel,&#xA;and quantum measurements.&lt;/p&gt;&#xA;&lt;h3 id=&#34;cha2:sec2:classical&#34;&gt;Classical digital logical circuit&lt;/h3&gt;&#xA;&lt;p&gt;Digital logic circuits are the foundational building blocks of classical&#xA;computing systems. They process classical bits by performing logical&#xA;operations through logic gates. In this subsection, we introduce the&#xA;essential components of digital logic circuits and their functionality,&#xA;followed by a discussion of how these classical circuits relate to&#xA;quantum circuits.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3.2 Quantum Kernel Machines</title>
      <link>http://localhost:1313/chapter3/2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter3/2/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 3.2 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;motivations-for-quantum-kernel-machines&#34;&gt;Motivations for quantum kernel machines&lt;/h3&gt;&#xA;&lt;p&gt;To effectively introduce quantum kernel machines, it is essential to&#xA;recognize the limitations of classical kernel machines. As discussed before, classical kernel machines rely on&#xA;manually tailored feature mappings, such as polynomials or radial basis&#xA;functions. However, these mappings may fail to capture the complex&#xA;patterns behind the dataset. Quantum kernel machines emerge as a&#xA;promising alternative, as they perform feature mapping using quantum&#xA;circuits, enabling them to explore exponentially larger feature spaces&#xA;that are otherwise infeasible for classical computation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4.2 Fault-tolerant Quantum Perceptron</title>
      <link>http://localhost:1313/chapter4/2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter4/2/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 4.2 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;The primary aim of advancing quantum machine learning is to harness the&#xA;computational advantages of quantum mechanics to enhance performance&#xA;across various learning tasks. These advantages manifest&#xA;in several ways, including reduced runtime, lower query complexity, and&#xA;improved sample efficiency compared to classical models. A notable&#xA;example of this is the quantum perceptron model [@kapoor2016quantum]. As&#xA;a [FTQC]{.sans-serif}-based QML algorithm grounded in the Grover search,&#xA;quantum perceptron offers a quadratic improvement in the query&#xA;complexity during the training over its classical counterpart. For&#xA;comprehensiveness, we first introduce the Grover search algorithm,&#xA;followed by a detailed explanation of the quantum perceptron model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5.2 Fault-tolerant Quantum Transformer</title>
      <link>http://localhost:1313/chapter5/2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter5/2/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 5.2 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this section, we move on to show an end-to-end transformer&#xA;architecture implementable on a quantum device, which includes all the&#xA;key building blocks introduced in&#xA;Chapter 5.1, and a discussion of the&#xA;potential runtime speedups of this quantum model. In particular, here we&#xA;focus on the &lt;em&gt;inference process&lt;/em&gt; in which a classical Transformer has&#xA;already been trained and is queried to predict the single next token.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tranformer</title>
      <link>http://localhost:1313/code/transformer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/transformer/</guid>
      <description>&lt;p&gt;We explain how self-attention works with a simple concrete example.&#xA;Consider a short sequence of three words: &amp;ldquo;The cat sleeps&amp;rdquo;.&lt;/p&gt;&#xA;&lt;p&gt;First, we convert each word into an embedding vector. For this toy&#xA;example, we use very small 4-dimensional embeddings:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sleeps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In self-attention, each word needs to &amp;ldquo;attend&amp;rdquo; to all other words in the&#xA;sequence. This happens through three key steps using learned weight&#xA;matrices ($W_q$, $W_k$, $W_v$) to transform the embeddings into queries,&#xA;keys, and values. When we multiply our word embeddings by these&#xA;matrices, we get&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2.3 Quantum Read-in and Read-out protocols</title>
      <link>http://localhost:1313/chapter2/3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter2/3/</guid>
      <description>&lt;p&gt;The terms &lt;strong&gt;quantum read-in&lt;/strong&gt; and &lt;strong&gt;read-out&lt;/strong&gt; refer to the processes of&#xA;transferring information between classical systems and quantum systems.&#xA;These are fundamental steps in the workflow of quantum machine learning, responsible for loading&#xA;data and extracting results.&lt;/p&gt;&#xA;&lt;p&gt;Quantum read-in and read-out pose significant bottlenecks in leveraging&#xA;quantum computing to address classical computational tasks. As&#xA;emphasized in [@aaronson2015read], while quantum algorithms can offer&#xA;exponential speed-ups in specific problem domains, these advantages can&#xA;be negated if the processes of loading classical data into quantum&#xA;systems (read-in) or extracting results from quantum systems (read-out)&#xA;are inefficient. Specifically, the high-dimensional nature of quantum&#xA;states and the constraints on measurement precision often lead to&#xA;overheads that scale poorly with problem size. These challenges&#xA;underscore the importance of optimizing quantum read-in and read-out&#xA;protocols to realize the full potential of quantum computing. Below is a&#xA;detailed introduction to quantum read-int and read-out protocols,&#xA;including the basic concept and several typical algorithms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3.3 Theoretical Foundations</title>
      <link>http://localhost:1313/chapter3/3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter3/3/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 3.3 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;theoretical-foundations-of-quantum-kernel-machines&#34;&gt;Theoretical Foundations of Quantum Kernel Machines&lt;/h2&gt;&#xA;&lt;p&gt;In this section, we take a step further to explore the theoretical&#xA;foundations of quantum kernels. Specifically, we focus on two crucial&#xA;aspects: the &lt;em&gt;expressivity&lt;/em&gt; and &lt;em&gt;generalization&lt;/em&gt; properties of quantum&#xA;kernel machines. As shown in&#xA;Figure 3.4,&#xA;these two aspects are essential for understanding the potential&#xA;advantages of quantum kernels over classical learning approaches and&#xA;their inherent limitations. For ease of understanding, this section&#xA;emphasizes the fundamental concepts necessary for evaluating the power&#xA;and limitation of quantum kernels instead of exhaustively reviewing all&#xA;theoretical results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4.3 Near-term quantum neural networks</title>
      <link>http://localhost:1313/chapter4/3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter4/3/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 4.3 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Following recent experimental breakthroughs in superconducting quantum&#xA;hardware&#xA;architectures [@arute2019quantum; @acharya2024quantum; @abughanem2024ibm; @gao2024establishing],&#xA;researchers have devoted considerable effort to developing and&#xA;implementing quantum machine learning algorithms optimized for current&#xA;and near-term quantum devices [@wang2024comprehensive]. Compared to&#xA;fault-tolerant quantum computers, these devices face two primary&#xA;limitations: quantum noise and circuit connectivity constraints.&#xA;Regarding quantum noise, state-of-the-art devices have single-qubit gate&#xA;error rates of $10^{-4} \sim 10^{-3}$ and two-qubit gate error rates of&#xA;approximately&#xA;$10^{-3} \sim 10^{-2}$ [@abughanem2024ibm; @gao2024establishing]. The&#xA;coherence time is around&#xA;$10^2 \mu s$ [@acharya2024quantum; @abughanem2024ibm; @gao2024establishing],&#xA;primarily limited by decoherence in noisy quantum channels. Regarding&#xA;circuit connectivity, most superconducting quantum processors employ&#xA;architectures that exhibit two-dimensional connectivity patterns and&#xA;their&#xA;variants [@acharya2024quantum; @abughanem2024ibm; @gao2024establishing].&#xA;Gate operations between non-adjacent qubits must be executed through&#xA;intermediate relay operations, leading to additional error accumulation.&#xA;To address these inherent limitations, the near-term quantum neural&#xA;network (QNN) framework has been proposed. Specifically, near-term QNNs&#xA;are designed to perform meaningful computations using quantum circuits&#xA;of limited depth.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5.3 Runtime Analysis with Quadratic Speedups</title>
      <link>http://localhost:1313/chapter5/3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter5/3/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 5.3 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;As Theorem 5.1 shows, the quantum transformer&#xA;uses ${\mathcal{\widetilde{O}}}(d N^2 \alpha_s\alpha_w)$ times the input&#xA;block encodings, where $\alpha_s$ and $\alpha_w$ are encoding factors&#xA;that significantly influence the overall complexity. We obtain this&#xA;final complexity on the basis of the following considerations: for&#xA;one-single layer transformer architecture, it includes one&#xA;self-attention, one feed-forward network, and two residual connection&#xA;with layer normalization, corresponding to the encoder-only or&#xA;decoder-only structure. Starting from the input assumption, for the index $j\in [\ell]$, we first&#xA;construct the block encoding of self-attention matrix. This output can be directly&#xA;the input of the quantum residual connection and layer normalization, which output is a state&#xA;encoding. The state encoding can directly be used&#xA;as the input of the feed-forward network. Finally, we put the output of the&#xA;feed-fordward network, which is a state encoding, into the residual&#xA;connection block. This is possible by noticing that state encoding is a&#xA;specific kind of the block encoding. Multiplying the query complexity of&#xA;these computational blocks, one can achieve the result. The detailed analysis of&#xA;runtime is referred to @guo2024quantumlinear2024&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2.4 Quantum Linear Algebra</title>
      <link>http://localhost:1313/chapter2/4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter2/4/</guid>
      <description>&lt;p&gt;We next introduce quantum linear algebra, a potent toolbox for designing&#xA;various FTQC-based algorithms. For clarity, we start with the&#xA;definition of block encoding, which is about how to implement&#xA;a matrix on the quantum computer. Based on this, we introduce some basic&#xA;arithmetic rules for block encodings, like the&#xA;multiplication, linear combination, and the Hadamard product. Finally, we introduce the quantum singular value&#xA;transformation method, which enables one to implement functions onto&#xA;singular values of block-encoded matrices.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3.4 Recent Advancements</title>
      <link>http://localhost:1313/chapter3/4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter3/4/</guid>
      <description>&lt;p&gt;The foundational concept of using quantum computers to evaluate kernel&#xA;functions, namely the concept of quantum kernels, was first explored by&#xA;@schuld2017implementing, who highlighted the fundamental differences&#xA;between quantum kernels and quantum support vector machines. Building on&#xA;this, @havlivcek2019supervised and @schuld2019quantum established a&#xA;connection between quantum kernels and parameterized quantum circuits&#xA;(PQCs), demonstrating their practical implementation. These works&#xA;emphasized the parallels between quantum feature maps and the classical&#xA;kernel trick. Since then, a large number of studies delved into figuring&#xA;out the potential of quantum kernels for solving practical real-world&#xA;problems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4.4 Theoretical Foundations of QNNs</title>
      <link>http://localhost:1313/chapter4/4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter4/4/</guid>
      <description>&lt;p&gt;&lt;em&gt;This content of this section corresponds to the Chapter 4.4 of our paper. Please refer to the original paper for more details.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;As a machine learning (ML) model, the primary goal of quantum neural&#xA;networks (QNNs) is to make accurate predictions on unseen data.&#xA;Achieving this goal depends on three key factors: expressivity,&#xA;generalization ability, and trainability. A thorough understanding of&#xA;these factors is crucial for evaluating the potential advantages and&#xA;limitations of QNNs compared to classical ML models. Instead of&#xA;providing an exhaustive review of all theoretical results, this section&#xA;focuses on near-term QNNs, emphasizing key conceptual insights into&#xA;their capabilities and limitations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5.4 Recent Advancements</title>
      <link>http://localhost:1313/chapter5/4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter5/4/</guid>
      <description>&lt;p&gt;Transformer architecture has profoundly revolutionized AI and has broad&#xA;impacts. As classical computing approaches its physical limitations, it&#xA;is important to ask how we can leverage quantum computers to advance&#xA;Transformers with better performance and energy efficiency. Besides the&#xA;fault-tolerant quantum Transformers, multiple works are&#xA;advancing this frontier from various perspectives.&lt;/p&gt;&#xA;&lt;p&gt;One aspect is to design novel quantum neural network architectures&#xA;introduced in Chapter 4 with the intuition from the Transformer,&#xA;especially the design of the self-attention block. In particular,&#xA;@li2023quantumselfattentionneuralnetworks proposes the quantum&#xA;self-attention neural networks and verifies their effectiveness with the&#xA;synthetic data sets of quantum natural language processing. There are&#xA;several follow-up works along this direction&#xA;[@Cherrat_2024; @evans2024learningsasquatchnovelvariational; @widdows2024quantumnaturallanguageprocessing].&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4.5 Recent Advancements</title>
      <link>http://localhost:1313/chapter4/5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter4/5/</guid>
      <description>&lt;p&gt;Quantum neural networks (QNNs) have emerged as a prominent paradigm in&#xA;quantum machine learning, demonstrating potential in both discriminative&#xA;and generative learning tasks. For discriminative tasks, QNNs utilize&#xA;high-dimensional Hilbert spaces to efficiently capture and represent&#xA;complex intrinsic relationships. In generative tasks, QNNs leverage&#xA;variational quantum circuits to generate complex probability&#xA;distributions that may exceed the capabilities of classical models.&#xA;While these approaches share common learning strategies, they each&#xA;introduce unique challenges and opportunities in terms of model design,&#xA;theoretical exploration, and practical implementation. In the remainder&#xA;of this section, we briefly review recent advances in QNNs. Interested&#xA;readers can refer to @ablayev2019quantum&#xA;[@li2022recent; @massoli2022leap] for a more comprehensive review.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Resources</title>
      <link>http://localhost:1313/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/resources/</guid>
      <description>&lt;p&gt;A list of materials, including textbooks, lecture notes, software platforms.&lt;/p&gt;&#xA;&lt;h2 id=&#34;textbooks&#34;&gt;Textbooks&lt;/h2&gt;&#xA;&lt;h2 id=&#34;open-source-libraries&#34;&gt;Open-source libraries&lt;/h2&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;p&gt;Introduction of team and team members.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2.5 Recent Advancements</title>
      <link>http://localhost:1313/chapter2/5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/chapter2/5/</guid>
      <description>&lt;p&gt;We end this chapter by discussing the recent advancements in efficiently&#xA;implementing fundamental components of quantum computing. For clarity,&#xA;we begin with a brief discussion of advanced quantum read-in and&#xA;read-out protocols, which are crucial for efficiently loading and&#xA;extracting classical data in the pipeline of quantum machine learning.&#xA;Next, we review the latest progress in quantum linear algebra.&lt;/p&gt;&#xA;&lt;h3 id=&#34;advanced-quantum-read-in-protocols&#34;&gt;Advanced quantum read-in protocols&lt;/h3&gt;&#xA;&lt;p&gt;Although conventional read-in protocols offer feasible solutions for&#xA;encoding classical data into quantum computers, they typically face two&#xA;key challenges that limit their broad applicability for solving&#xA;practical learning problems. To address these limitations, initial&#xA;efforts have been made to develop more advanced quantum read-in&#xA;protocols.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
