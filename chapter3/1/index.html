<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Chapter 3.1 Classical Kernel - Quantum Machine Learning Tutorial</title>
<meta name="description" content="A Hands-on Tutorial for Machine Learning Practitioners and Researchers">
<meta name="generator" content="Hugo 0.140.2">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/chapter3/1/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:url" content="http://localhost:1313/chapter3/1/">
  <meta property="og:site_name" content="Quantum Machine Learning Tutorial">
  <meta property="og:title" content="Chapter 3.1 Classical Kernel">
  <meta property="og:description" content="This content of this section corresponds to the Chapter 3.1 of our paper. Please refer to the original paper for more details.
Motivations for classical kernel machines Before delving into kernel machines, it is essential to first understand the motivation behind kernel methods. In many machine learning tasks, particularly in classification, the goal is to find a decision boundary that best separates different classes of data. When the data is linearly separable, this boundary can be represented as a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions), as illustrated in the following Figure. Mathematically, given an input space $\mathcal{X}\subset \mathbb{R}^d$ with $d\ge 1$ and a target or output space $\mathcal{Y}={&#43;1,-1}$, we consider a training dataset $\mathcal{D}={(\bm{x}^{(i)}, {y}^{(i)})}_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$ where each data point $\bm{x}^{(i)} \in \mathcal{X}$ is associated with a label ${y}^{(i)} \in \mathcal{Y}$. For the dataset to be linearly separable, there must exist a vector $\bm{w} \in \mathbb{R}^{d}$ and a bias term $b\in \mathbb{R}$ such that $$\forall i\in [n], \quad {y}^{(i)}(\bm{w}^{\top} \cdot \bm{x}^{(i)}&#43;b)\ge 0.$$ This means that a hyperplane defined by $(\bm{\omega},b)$ can perfectly separate the two classes.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chapter3">
    <meta property="og:image" content="http://localhost:1313/images/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/og-image.png">
  <meta name="twitter:title" content="Chapter 3.1 Classical Kernel">
  <meta name="twitter:description" content="This content of this section corresponds to the Chapter 3.1 of our paper. Please refer to the original paper for more details.
Motivations for classical kernel machines Before delving into kernel machines, it is essential to first understand the motivation behind kernel methods. In many machine learning tasks, particularly in classification, the goal is to find a decision boundary that best separates different classes of data. When the data is linearly separable, this boundary can be represented as a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions), as illustrated in the following Figure. Mathematically, given an input space $\mathcal{X}\subset \mathbb{R}^d$ with $d\ge 1$ and a target or output space $\mathcal{Y}={&#43;1,-1}$, we consider a training dataset $\mathcal{D}={(\bm{x}^{(i)}, {y}^{(i)})}_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$ where each data point $\bm{x}^{(i)} \in \mathcal{X}$ is associated with a label ${y}^{(i)} \in \mathcal{Y}$. For the dataset to be linearly separable, there must exist a vector $\bm{w} \in \mathbb{R}^{d}$ and a bias term $b\in \mathbb{R}$ such that $$\forall i\in [n], \quad {y}^{(i)}(\bm{w}^{\top} \cdot \bm{x}^{(i)}&#43;b)\ge 0.$$ This means that a hyperplane defined by $(\bm{\omega},b)$ can perfectly separate the two classes.">

  <meta itemprop="name" content="Chapter 3.1 Classical Kernel">
  <meta itemprop="description" content="This content of this section corresponds to the Chapter 3.1 of our paper. Please refer to the original paper for more details.
Motivations for classical kernel machines Before delving into kernel machines, it is essential to first understand the motivation behind kernel methods. In many machine learning tasks, particularly in classification, the goal is to find a decision boundary that best separates different classes of data. When the data is linearly separable, this boundary can be represented as a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions), as illustrated in the following Figure. Mathematically, given an input space $\mathcal{X}\subset \mathbb{R}^d$ with $d\ge 1$ and a target or output space $\mathcal{Y}={&#43;1,-1}$, we consider a training dataset $\mathcal{D}={(\bm{x}^{(i)}, {y}^{(i)})}_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$ where each data point $\bm{x}^{(i)} \in \mathcal{X}$ is associated with a label ${y}^{(i)} \in \mathcal{Y}$. For the dataset to be linearly separable, there must exist a vector $\bm{w} \in \mathbb{R}^{d}$ and a bias term $b\in \mathbb{R}$ such that $$\forall i\in [n], \quad {y}^{(i)}(\bm{w}^{\top} \cdot \bm{x}^{(i)}&#43;b)\ge 0.$$ This means that a hyperplane defined by $(\bm{\omega},b)$ can perfectly separate the two classes.">
  <meta itemprop="wordCount" content="677">
  <meta itemprop="image" content="http://localhost:1313/images/og-image.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '\\[', right: '\\]', display: true }, { left: '$', right: '$', display: false }, { left: '\\(', right: '\\)', display: false } ] });"></script>
</head>
<body>

<div class="container"><header>
<h1>Quantum Machine Learning Tutorial</h1>
<p class="description">A Hands-on Tutorial for Machine Learning Practitioners and Researchers</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Chapter 3.1 Classical Kernel</h1>
<p><em>This content of this section corresponds to the Chapter 3.1 of our paper. Please refer to the original paper for more details.</em></p>
<h3 id="motivations-for-classical-kernel-machines">Motivations for classical kernel machines</h3>
<p>Before delving into kernel machines, it is essential to first understand
the motivation behind kernel methods. In many machine learning tasks,
particularly in classification, the goal is to find a decision boundary
that best separates different classes of data. When the data is linearly
separable, this boundary can be represented as a straight line (in 2D),
a plane (in 3D), or a hyperplane (in higher dimensions), as illustrated
in the following Figure. Mathematically, given an input space
$\mathcal{X}\subset \mathbb{R}^d$ with $d\ge 1$ and a target or output
space $\mathcal{Y}={+1,-1}$, we consider a training dataset
$\mathcal{D}={(\bm{x}^{(i)}, {y}^{(i)})}_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$
where each data point $\bm{x}^{(i)}  \in \mathcal{X}$ is associated with
a label ${y}^{(i)}  \in \mathcal{Y}$. For the dataset to be linearly
separable, there must exist a vector $\bm{w} \in \mathbb{R}^{d}$ and a
bias term $b\in \mathbb{R}$ such that
$$\forall i\in [n], \quad {y}^{(i)}(\bm{w}^{\top} \cdot \bm{x}^{(i)}+b)\ge 0.$$
This means that a hyperplane defined by $(\bm{\omega},b)$ can perfectly
separate the two classes.</p>
<blockquote>
<p>Throughout the whole tutorial, we interchangeably use the following
notations to denote the inner product of two vectors $\bm{a}$ and
$\bm{b}$, i.e., [complete. $\langle \cdot, \cdot \rangle$,
$\braket{\cdot|\cdot}$, $\bm{a}\cdot \bm{b}$,
$\bm{a}^{\dagger} \bm{b}$]</p>
</blockquote>
<figure style="text-align:center;">
  <img src="../../images/chapter3/hyperplane.png" alt="图片描述" style="max-width:80%; height:auto;">
</figure>
<p>However, in real-world scenarios, data is often not linearly separable,
as shown in the Figure(b). The decision boundary required to
separate classes may be curved or highly complex. Traditional linear
models struggle with such non-linear data because they are inherently
limited to creating only linear decision boundaries. This limitation
highlights the need for more flexible approaches.</p>
<p>To address the challenge of non-linear data, one effective strategy is
to transform the input data into a higher-dimensional space where the
data may become linearly separable. This transformation is known as
<em>feature mapping</em>, denoted by $$
\bm{\phi}:\bm{x} \to \phi(\bm{x})\in \mathbb{R}^D,$$ where the
original input space $\mathcal{X}$ is mapped to a higher-dimensional
feature space $\mathbb{R}^D$ with $D\ge d$. The idea is that, in this
higher-dimensional space, complex patterns in the original data can be
more easily identified using linear models.</p>
<p>However, explicitly computing the feature map $\bm{\phi}(\bm{x})$ can be computationally expensive,
especially if the feature space is high-dimensional or even
infinite-dimensional. Fortunately, many machine learning algorithms for
tasks like classification or regression depend primarily on the inner
product between data points. In the feature space, this inner
product is given by
$\braket{\bm{\phi}(\bm{x}^{(i)}),\bm{\phi}(\bm{x}^{(j)})}$.</p>
<p>To circumvent the computational cost of explicitly calculating the
feature map, we can use a <em>kernel function</em>. A kernel function
$k(\bm{x}^{(i)}, \bm{x}^{(j)})$ is defined as
$$
k(\bm{x}^{(i)}, \bm{x}^{(j)})=\braket{\bm{\phi}(\bm{x}^{(i)}),\bm{\phi}(\bm{x}^{(j)})}.$$
This allows us to compute the inner product in the higher-dimensional
feature space indirectly, without ever having to compute
$\bm{\phi}(\bm{x})$ explicitly. This approach is commonly known as the
<em>kernel trick</em>.</p>
<p>By using the kernel function directly within algorithms, we avoid the
computational overhead of working in a high-dimensional space. The
collection of kernel values for a dataset forms the <em>kernel matrix</em> (or
<em>Gram matrix</em>), where each entry is given by
$k(\bm{x}^{(i)}, \bm{x}^{(j)})$. This matrix is central to many
kernel-based algorithms, as it captures the pairwise similarities
between all training data points.</p>
<blockquote>
<p>Throughout this tutorial, we use the notations $\mathcal{O}$ and
$\Omega$ to represent the asymptotic upper and lower bounds,
respectively, on the growth rate of a term, ignoring constant factors
and lower-order terms.</p>
</blockquote>
<h3 id="kernel-construction">Kernel construction</h3>
<p>To utilize the kernel trick in machine learning algorithms, it is
essential to construct valid kernel functions. One approach is to start
with a feature mapping $\bm{\phi}(\bm{x})$ and then derive the
corresponding kernel. For a one-dimensional input space, the kernel
function is defined as
$$k(x, x&rsquo;) = \langle \bm{\phi}(\bm{x})^{\top}, \bm{\phi}(\bm{x}&rsquo;) \rangle = \sum_{i=1}^D \langle \bm{\phi}_i(\bm{x}), \bm{\phi}_i(\bm{x}&rsquo;) \rangle,$$
where $\bm{\phi}_i(\bm{x})$ are the basis functions of the feature map.</p>
<p>Alternatively, kernels can be constructed directly without explicitly
defining a feature map. In this case, we must ensure that the chosen
function is a valid kernel, meaning it corresponds to an inner product
in some (possibly infinite-dimensional) feature space. The validity of a
kernel function is guaranteed by Mercer&rsquo;s condition.</p>
<div class="edit-meta">

<br><a href="https://github.com/thingsym/hugo-theme-techdoc/edit/master/chapter3/1.md" class="edit-page"><i class="fas fa-pen-square"></i>&nbsp;Edit on GitHub</a></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/chapter3/" title="Chapter 3 Quantum Kernel Methods"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Chapter 3 Quantum Kernel Methods</a>
<a class="nav nav-next" href="http://localhost:1313/chapter3/2/" title="Chapter 3.2 Quantum Kernel Machines">Next - Chapter 3.2 Quantum Kernel Machines <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="http://localhost:1313/">Home</a></li>

<li class=""><a href="http://localhost:1313/getting-started/">Getting Started</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter1/">Chapter 1 Introduction of QML</a>
  
</li>

<li class=""><a href="http://localhost:1313/chapter2/">Chapter 2 Basis of Quantum Computing</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter2/1/">Chapter 2.1 From Classical Bits to Quantum Bits</a></li>
<li class=""><a href="http://localhost:1313/chapter2/2/">Chapter 2.2 From Digital Logical Circuit to Quantum Circuit Model</a></li>
<li class=""><a href="http://localhost:1313/chapter2/3/">Chapter 2.3 Quantum Read-in and Read-out protocols</a></li>
<li class=""><a href="http://localhost:1313/chapter2/4/">Chapter 2.4 Quantum Linear Algebra</a></li>
<li class=""><a href="http://localhost:1313/chapter2/5/">Chapter 2.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class="parent"><a href="http://localhost:1313/chapter3/">Chapter 3 Quantum Kernel Methods</a>
  
<ul class="sub-menu">
<li class="active"><a href="http://localhost:1313/chapter3/1/">Chapter 3.1 Classical Kernel</a></li>
<li class=""><a href="http://localhost:1313/chapter3/2/">Chapter 3.2 Quantum Kernel Machines</a></li>
<li class=""><a href="http://localhost:1313/chapter3/3/">Chapter 3.3 Theoretical Foundations</a></li>
<li class=""><a href="http://localhost:1313/chapter3/4/">Chapter 3.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter4/">Chapter 4 Quantum Neural Networks</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter4/1/">Chapter 4.1 Classical Neural Networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/2/">Chapter 4.2 Fault-tolerant Quantum Perceptron</a></li>
<li class=""><a href="http://localhost:1313/chapter4/3/">Chapter 4.3 Near-term quantum neural networks</a></li>
<li class=""><a href="http://localhost:1313/chapter4/4/">Chapter 4.4 Theoretical Foundations of QNNs</a></li>
<li class=""><a href="http://localhost:1313/chapter4/5/">Chapter 4.5 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/chapter5/">Chapter 5 Quantum Transformer</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/chapter5/1/">Chapter 5.1 Classical Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/2/">Chapter 5.2 Fault-tolerant Quantum Transformer</a></li>
<li class=""><a href="http://localhost:1313/chapter5/3/">Chapter 5.3 Runtime Analysis with Quadratic Speedups</a></li>
<li class=""><a href="http://localhost:1313/chapter5/4/">Chapter 5.4 Recent Advancements</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/code/">Code Examples</a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/code/data-encode/">Data Encoding</a></li>
<li class=""><a href="http://localhost:1313/code/kernel-mnist/">Classification on MNIST</a></li>
<li class=""><a href="http://localhost:1313/code/classifier/">Quantum Classifier</a></li>
<li class=""><a href="http://localhost:1313/code/patch-qgan/">Quantum Patch GAN</a></li>
<li class=""><a href="http://localhost:1313/code/transformer/">Tranformer</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
